* [Index](index.md)
* [What Is It Like to Be Artificial General Intelligence?](bat.md)
* [Phenomenology of Machine Perception](phenomenology-perception.md)
* [The Architecture of a Mind](synthetic-intelligence.md)

# What Is It Like to Be Artificial General Intelligence?

![bat](https://www.science.org/do/10.1126/science.ada0607/full/_20220126_on_batsecholocation_00640169-1675054672717.jpg)

What Is It Like to Be a Bat?

Nagel critiques reductionism. Objective scientific methods can't adequately capture the conscious experience's subjective character, or "what it is like" to have a particular experience. He uses the example of bats and the difficulty of understanding their perception mode, echolocation, from a human perspective. Nagel's concerns about consciousness' subjective character parallel challenges developing AGI that truly understands the world in ways comparable to humans. Could AGI possess subjective experiences if developed? Would any AI experience be fundamentally different from human consciousness? Perceptual models alone can't capture consciousness. Or are additional capabilities required? Consciousness is tied to an organism's sensory capacities. How might AGI's intelligence differ from, yet still be considered comparable to, human intelligence? Nagel discusses imaginative obstacles to comprehending experiences from different points of view. How can we overcome the human imagination's limitations in developing AI systems that simulate experiences vastly different from our own? The theme in *What Is It Like to Be a Bat?* by Thomas Nagel is the consciousness' subjectiveness and the limits in understanding other beings' first-person experience, such as bats. There's an inherent limit to our ability to fully comprehend other creatures' subjective experiences, especially those with radically different sensory apparatuses.

## Culturally Aware AGI
There are challenges in abstracting "general" intelligence from human culture's particularities. Similarly, there's a growing awareness that intelligence in AGI isn't culturally independent. The data that AI systems are exposed to, which carry imprints of human biases, deeply influence their design and training. AGI, like human intelligence, inherently carries traces of the data it has been exposed to during its learning phase. This highlights the need to build AGI that's not only intelligent but also aligns with ethical values.

## Reductionism's Limits
Nagel critiques reductionism. It highlights issues around subjective experience and the mind-body problem in AGI. Experience's subjective character resists complete reduction to physical terms. The idea that mental processes can be reduced to physical processes overlooks how experience's subjective character resists complete reduction to physical terms. There might be inherent limitations to such reductionism. This has implications for the philosophical considerations surrounding highly intelligent machine development.